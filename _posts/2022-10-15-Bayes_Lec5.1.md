---
layout: post
title: "Bayes-Lec.1 先验分布的选取"
subtitle: "Selection of Prior Distribution"
author: "Watthu"
header-style: text
mathjax: true
tags:
  - 贝叶斯分析
---

## 主观先验

主观概率是人们根据经验对事件发生机会的个人信念。当感兴趣未知参数是离散取值时，常用**相对似然法**、**专家意见方法**和**利用历史资料**等方法确定先验概率。下面主要就专家意见方法作详细分析。

在实际问题中，存在多个独立地、具有重要信息的专家，将专家信息转化为概率十分重要。**概率启发（probability elicitation）方法**就是用来将专家意见转化为概率的方法。

- 设立并准备启发：选择专家、训练专家、识别问题的哪些方面要启发，等等；
- 专家意见的启发：对目标问题的这些方面得出专家们分布的特定特征量。
- 分布拟合：对这些特征量拟合一个概率分布。
- 评估充足性：评估启发过程的充足性，否则返回到第二步中得出更多总结统计量。

专家多使用启发式方法来进行概率判断，这会产生偏差和不连贯性。专家训练的一个重要部分就是尽量使专家认识到这些偏差，从而消除它们。

- 动机偏差：对参与先验分布选取的动机不同。
- 认知偏差
    - 可得性偏差：由于最近的影响或感情上、特征上的特殊印象，容易高估事件出现的可能性。
    - 锚定与调整性偏差：通常利用某个参照点和锚来降低模糊性，然后再通过一定的调整来得出最后的结论。
    - 代表性偏差：做决定时借鉴要判断事件本身或事件的同类事件以往的经验来进行。

一般而言，直接询问专家关于先验分布参数的值是不合适的，通常询问专家一些可观测量再通过数值或统计计算来得到分布参数的值。

在评估专家预报质量方面，常用的准则如下：

- 诚实：我们希望专家讲实话（这通常会采用相应的奖励机制）；
- 一贯性：专家的预报应该满足概率法则；
- 一致性：如果专家没有新信息，那么专家的预报不变；
- 校正：如果专家说下雨的概率为0.5，那么50%的日子里是下雨的；
- 信息性：如果在一个地方一年大约有50天下雨，那么一个专家说明天下雨的概率是50/364，则每天都不是很有信息。

对专家质量的数值度量，有许多研究者做了相应的研究，可以参考相关的文章（De Groot and Fienberg (1982)、Cooke等人 (1988)、Cooke (1991)）。

## 利用边缘分布确定先验分布

前面在计算常见分布参数的后验分布时，已经多次提到边缘分布的概念，这里正式给出它的定义。实际上，也是十分容易理解的。

---
**定义(边缘分布)** 设随机变量 $X$ 有概率函数 $f(x\mid\theta)$，$\theta$ 有先验分布函数 $\pi(\theta)$，则随机变量 $X$ 的边缘分布定义为
<center>
$\displaystyle m(x)=\int_{\Theta}f(x\mid\theta)\text{d}F^\pi(\theta)=\left\{\begin{array}{ll}
	\sum\limits_{i}f(x\mid\theta_i)\pi(\theta_i), & \text{当}~\theta~\text{为离散型随机变量},\\
	\int_{\Theta}f(x\mid\theta)\pi(\theta)\text{d}\theta, & \text{当}~\theta~\text{为连续型随机变量}.
	\end{array}\right.$
</center>

---

当先验分布含有未知的超参数 $\lambda$ 时，记 $\pi(\theta)=\pi(\theta\mid\lambda)$，则边缘分布 $m(x)$ 也依赖于 $\lambda$，可记 $m(x)=m(x\mid\lambda)$。

为了说明边缘分布的统计意义，下面引入混合分布的概念。设随机变量 $X$ 以概率 $p$ 在总体 $F(x\mid\theta_1)$ 中取值，以概率 $1-p$ 在总体 $F(x\mid\theta_2)$ 中取值，则 $X$ 的混合分布函数为
<center>
$F(x)=pF(x\mid\theta_1)+(1-p)F(x\mid\theta_2).$
</center>

而 $p$ 和 $1-p$ 可视作 $\theta$ 的分布，即 $\mathbb P(\theta=\theta_1)=p$，$\mathbb P(\theta=\theta_2)=1-p$，则上式为边缘分布的表示形式。由此可以看到，边缘分布是混合分布的推广。

![](/img/in_post/Bayes/BayesLec5_1_1.png)

### 选择先验分布的ML-II方法

设随机变量 $X$ 的概率分布为 $f(x\mid\theta)$，$\theta$ 的先验分布是 $\pi(\theta)$，则它的边缘分布为 $m(x)$，与先验分布 $\pi(\theta)$ 有关，记为 $m(x)=m(x|\pi)$。

当观测到样本 $X=x$，若有2个先验分布 $\pi_1$ 和 $\pi_2$ 使得 $m(x\mid\pi_1)>m(x\mid\pi_2)$，则认为样本 $X=x$ 由分布 $m(x|\pi_1)$ 产生。这一思想与经典统计方法中的极大似然原理类似，由此引入**ML-II先验方法**。

---
**定义(ML-II先验)** 设 $\Gamma$ 为所考虑的先验类，若存在 $\hat{\pi}\in\Gamma$，有了样本 $\pmb x=(x_1,\dots,x_n)$ 后，使得
<center>
$\displaystyle m(\pmb x\mid\hat{\pi})=\sup_{\pi\in\Gamma}m(\pmb x\mid\pi)=\sup_{\pi\in\Gamma}\prod_{i=1}^nm(x_i\mid\pi),$
</center>

则称 $\hat{\pi}$ 为**类型II中最大似然先验**，或简称**ML-II先验**。

---

若样本 $\pmb X=(X_1,\dots,X_n)$ 所涉及的先验密度函数的形式已知，未知的仅是其中的超参数，即先验密度的类 $\Gamma$ 可表示为
<center>
$\Gamma=\left\{\pi(\theta\mid\lambda),\lambda~\text{为超参数(或超参数向量)},~\lambda\in\Lambda\right\}.$
</center>

此时寻求ML-II先验较为简单，只要求出这样的 $\hat{\lambda}$，使得
<center>
$\displaystyle m(\pmb x\mid\hat{\lambda})=\sup_{\lambda\in\Lambda}m(\pmb x\mid\lambda)=\sup_{\lambda\in\Lambda}\prod_{i=1}^nm(x_i\mid\lambda),$
</center>

则先验分布 $\pi(\theta\mid\hat{\lambda})$ 即为所确定的先验分布，此时称 $\hat{\lambda}$ 为ML-II超参数。

![](/img/in_post/Bayes/BayesLec5_1_2.png)

### 选择先验分布的矩方法

当先验分布 $\pi(\theta\mid\lambda)$ 的形式已知，但含有未知的超参数 $\lambda$ 时，可利用先验分布的矩与边缘分布的矩之间的关系寻求超参数 $\lambda$ 的估计量 $\hat{\lambda}$，从而获得先验分布 $\pi(\theta\mid\hat{\lambda})$。具体步骤如下：

1. 计算样本分布 $F(x\mid\theta)$ 的期望 $\mu(\theta)$ 和方差 $\sigma^2(\theta)$，即

    <center>
    $\begin{array}{rl}
    \mu(\theta) =&\!\!\!\!\displaystyle\mathcal E[X\mid\theta]=\int_{\mathscr{X}}x\text{d}F(x\mid\theta),\\
    \sigma^2(\theta) =&\!\!\!\!\displaystyle\mathcal E[(X-\mu(\theta))^2\mid\theta]=\int_{\mathscr{X}}(x-\mu(\theta))^2\text{d}F(x\mid\theta).
    \end{array}$
    </center>

2. 计算边缘密度 $m(x)=m(x\mid\lambda)$ 的期望 $\mu_m(\lambda)$ 和方差 $\sigma_m^2(\lambda)$，即

    <center>
    $\begin{array}{rl}
    \mu_m(\lambda) =&\!\!\!\!\displaystyle\int_{\mathscr{X}}xm(x\mid\lambda)\text{d}x=\int_{\mathscr{X}}\int_\Theta xf(x\mid\theta)\pi(\theta\mid\lambda)\text{d}\theta\text{d}x\\
    =&\!\!\!\!\displaystyle\int_\Theta\mu(\theta)\pi(\theta\mid\lambda)\text{d}\theta=\mathbb E[\mu(\theta)\mid\lambda],\\
    \sigma_m^2(\lambda) =&\!\!\!\!\displaystyle\int_{\mathscr{X}}(x-\mu_m(\lambda))^2m(x\mid\lambda)\text{d}x=\int_{\mathscr{X}}\int_\Theta(x-\mu_m(\lambda))^2f(x\mid\theta)\pi(\theta\mid\lambda)\text{d}\theta\text{d}x\\
    =&\!\!\!\!\displaystyle\int_\Theta\mathbb E[(x-\mu_m(\lambda))^2\mid\theta]\pi(\theta\mid\lambda)\text{d}\theta\\
    =&\!\!\!\!\displaystyle\int_\Theta\big(\sigma^2(\theta)+[\mu(\theta)-\mu_m(\lambda)]^2\big)\pi(\theta\mid\lambda)\text{d}\theta.\\
    =&\!\!\!\!\displaystyle\mathbb E[\sigma^2(\theta)\mid\lambda]+\mathbb E\big[[\mu(\theta)-\mu_m(\lambda)]^2\mid\lambda\big].
    \end{array}$
    </center>

3. 当先验分布只含有2个超参数 $\pmb\lambda=(\lambda_1,\lambda_2)$ 时，用

    <center>
    $\displaystyle\hat{\mu}_m=\overline{X}=\frac{1}{n}\sum_{i=1}^nX_i$ 和 $\displaystyle\hat{\sigma}_m^2=S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2$
    </center>

    分别作为 $\mu_m(\pmb\lambda)$ 和 $\sigma_m^2(\pmb\lambda)$ 的估计。通过求解方程组
    <center>
    $\left\{\begin{array}{l}
	\hat{\mu}_m=\mathbb E[\mu(\theta)\mid\pmb\lambda],\\
	\hat{\sigma}_m^2=\mathbb E[\sigma^2(\theta)\mid\lambda]+\mathbb E\big[[\mu(\theta)-\mu_m(\pmb\lambda)]^2\mid\pmb\lambda\big].
	\end{array}\right.$
    </center>
    
    可以得到超参数的估计 $\hat{\pmb\lambda}=(\hat{\lambda}_1,\hat{\lambda}_2)$，从而得到先验分布的估计 $\pi(\theta\mid\hat{\pmb\lambda})$。
    
![](/img/in_post/Bayes/BayesLec5_1_3.png)

![](/img/in_post/Bayes/BayesLec5_1_4.png)

## 客观贝叶斯方法

### 均匀先验

Bayes（1763）和 Laplace（1812）选择**均匀先验**作为一般的无信息先验，其合理性的根据是不充分推理原则。这在有限维问题中没有问题，但是当参数空间是无界连续时，这种先验分布是不正常的，需要引入广义先验密度的概念。

---
**定义(广义先验密度)** 设随机变量 $X\sim f(x\mid\theta)$，$\theta\in\Theta$。若 $\theta$ 的先验分布 $\pi(\theta)$ 满足条件：

1. $\pi(\theta)\geq0$ 且 $\displaystyle\int_\Theta\pi(\theta)\text{d}\theta=\infty$；
2. 后验密度 $\pi(\theta\mid x)$ 是正常密度函数。

则称 $\pi(\theta)$ 为**广义先验密度**。

---

均匀先验的一个重要问题是其缺乏变换不变性，例如设 $\pi(\theta)\propto1$，令 $\phi=1/\theta$，则意味着 $\phi$ 的先验分布为 $\pi(\phi)\propto1/\phi^2$，就不是均匀分布了。因此使用均匀分布表示无信息是不一致的。

在Lec2.1节中已经介绍了关于位置参数族和尺度参数族的概念，这里关于它们的无信息先验选择不再赘述。下面提供几个例题供读者回顾理解相关概念。

![](/img/in_post/Bayes/BayesLec5_1_5.png)

![](/img/in_post/Bayes/BayesLec5_1_6.png)

### Jeffreys先验

Jeffreys先验提供了一个具有变换不变性质的先验分布。

---
**定义(Jeffreys先验)** 设 $X\sim f(\cdot\mid\theta)$，$\theta$ 为未知参数。定义 $\theta$ 的**Jeffreys先验**为
<center>
$\pi(\theta)\propto\sqrt{\text{det}~I(\theta)},$
</center>

其中 $I(\theta)$ 为Fisher信息阵。

---

下面这个定理揭示了Jeffreys先验的变换不变性：

---
**定理.** 设 $\phi=h(\theta)$ 为一单调函数，$\pi(\theta)\propto\sqrt{\text{det}~I(\theta)}$ 为 $\theta$ 的Jeffreys先验，则 $\phi$ 的Jeffreys先验分布为
<center>
$\pi(\phi)\propto\sqrt{\text{det}~I(\phi)}$，i.e. $[I(\phi)]^{1/2}=[I(\theta)]^{1/2}\Big\vert\dfrac{\text{d}\theta}{\text{d}\phi}\Big\vert,$
</center>

其中 $I(\phi)$ 为 $X$ 的分布重参数化为 $\phi$ 后的Fisher信息阵。

**证明.** 由微分的链式法则可知
<center>
$\dfrac{\text{d}^2\log f(x\mid\phi)}{\text{d}\phi^2}=\dfrac{\text{d}^2\log f(x\mid\phi)}{\text{d}\theta^2}\cdot\bigg(\dfrac{\text{d}\theta}{\text{d}\phi}\bigg)^2+\dfrac{\text{d}\log f(x\mid\theta)}{\text{d}\theta}\cdot\dfrac{\text{d}^2\theta}{\text{d}\phi^2}.$
</center>

因此
<center>
$\begin{array}{rl}
I(\phi) =&\!\!\!\! -\mathcal E_{X\vert\phi}\left[\dfrac{\text{d}^2\log f(X\mid\phi)}{\text{d}\phi^2}\right]\\
=&\!\!\!\! -\mathcal E_{X\vert\phi}\left[\dfrac{\text{d}^2\log f(X\mid\phi)}{\text{d}\theta^2}\right]\cdot\bigg(\dfrac{\text{d}\theta}{\text{d}\phi}\bigg)^2-\mathcal E_{X\vert\phi}\left[\dfrac{\text{d}\log f(x\mid\theta)}{\text{d}\theta}\right]\cdot\dfrac{\text{d}^2\theta}{\text{d}\phi^2}\\
=&\!\!\!\! I(\theta)\cdot\bigg(\dfrac{\text{d}\theta}{\text{d}\phi}\bigg)^2.
\end{array}$
</center>

---

![](/img/in_post/Bayes/BayesLec5_1_7.png)

对于多元参数下的Jeffreys先验，其计算相对复杂，但核心仍然是计算Fisher信息阵。

![](/img/in_post/Bayes/BayesLec5_1_8.png)

### 最大熵先验

最大熵先验的基本思想是在只有部分信息的情况下找一个最小的有信息先验分布。

首先考虑离散情形下的最大熵先验。

---
**定义(离散熵)** 设 $\theta$ 为一元离散变量，$p(\theta)$ 为其分布，则称
<center>
$\displaystyle e(p)=-\sum_{\theta_i\in\Theta}p(\theta_i)\log p(\theta_i)$
</center>

为其分布的**（离散）熵（entropy）**。

---

- 如果 $p(\theta_i)=1$ 对某个 $\theta_i\in\Theta$ 成立，则 $e(p)=0$，即为零不确定，或最小熵；
- 如果 $p(\theta_i)=1/\vert\Theta\vert$，即均匀分布，则

    <center>
    $\displaystyle e(p)=-\sum_{\theta_i\in\Theta}\dfrac{1}{\vert\Theta\vert}\log\dfrac{1}{\vert\Theta\vert}=\log\vert\Theta\vert,$
    </center>
    即为最大熵。

假设现在我们对参数具有部分信息：
<center>
$\displaystyle\mathbb E[g_k(\theta)]=\sum_{\theta_i\in\Theta}p(\theta_i)g_k(\theta_i)=\mu_k,\quad k=1,\dots,m.$
</center>

则最大熵先验为
<center>
$p(\theta_i)=\dfrac{\exp\left\{\sum\limits_{k=1}^m\lambda_kg_k(\theta_i)\right\}}{\sum\limits_{\theta_j\in\Theta}\exp\left\{\sum\limits_{k=1}^m\lambda_kg_k(\theta_j)\right\}},$
</center>

其中 $\lambda_k$ 可以从信息中确定。

![](/img/in_post/Bayes/BayesLec5_1_9.png)

下面考虑连续情形下的最大熵先验。

---
**定义(连续熵)** 设 $\theta$ 为一元连续变量，$\pi(\theta)\text{d}\mu(\theta)$ 为其分布，则称
<center>
$\displaystyle e(\pi)=-\int_\Theta\pi(\theta)\log\pi(\theta)\text{d}\mu(\theta)$
</center>

为依赖于测度 $\mu$ 的**连续熵**。

---

显然，从定义出发推广到连续变量比较复杂。一种可行的方法是定义
<center>
$\displaystyle e(\pi)=-\int_\Theta\pi(\theta)\log\dfrac{\pi(\theta)}{\pi_0(\theta)}\text{d}\theta,$
</center>

其中 $\pi_0(\theta)$ 为 $\theta$ 的Jeffreys先验。于是在给定信息 $\mathbb E[g_k(\theta)]=\mu_k$，$k=1,\dots,m$ 下，最大熵先验为
<center>
$\pi(\theta)=\dfrac{\pi_0(\theta)\exp\left\{\sum\limits_{k=1}^m\lambda_kg_k(\theta)\right\}}{\int_\Theta\pi_0(\theta)\exp\left\{\sum\limits_{k=1}^m\lambda_kg_k(\theta)\right\}\text{d}\theta},$
</center>

其中 $\lambda_k$ 满足给定信息。

![](/img/in_post/Bayes/BayesLec5_1_10.png)

不幸的是，最大熵先验在一些情况下可能不存在。例如，如果 $X\mid\mu\sim N(\mu,1)$，固定先验均值 $\mathbb E(\mu)=m$，由于Jeffreys先验为 $\pi(\mu)\propto1$，因此最大熵先验为
<center>
$\pi(\mu)=\dfrac{\exp(\lambda_1\mu)}{\int_{-\infty}^\infty\exp(\lambda_1\mu)\text{d}\mu}.$
</center>
该积分无解。

### Reference先验

Reference先验是一类最一般的方法，它基于最大化一个专家提供的参数的期望信息，其基本思想是在有观测数据后，使得先验分布和后验分布之间的K-L距离最大。在一维情形下，Reference先验和Jeffreys先验是相同的；当模型中存在多余参数时，二者是不一样的。

---
**定义(K-L距离)** 设有两个概率密度函数 $p(x)$ 和 $q(x)$，它们的K-L距离定义为
<center>
$KL(p(x),q(x))=\mathbb E_p[\log(p/q)]=\int p(x)\log\bigg(\dfrac{p(x)}{q(x)}\bigg)\text{d}x.$
</center>

---

K-L距离可以看作是连续型随机变量的熵，其值越大表示先验信息越少。

---
**定义(Reference先验)** 设样本 $\pmb X=(X_1,\dots,X_n)$ 的分布族为 $\{f(\pmb x\mid\pmb\theta),\pmb\theta\in\pmb\Theta\}$，其中 $\pmb\theta$ 为参数（向量），$\pmb\Theta$ 为参数空间；$\pmb\theta$ 的先验分布为 $\pi(\pmb\theta)$。令
<center>
$\mathscr{P}=\{\pi(\pmb\theta)>0:\int_\Theta\pi(\pmb\theta\mid\pmb x)\text{d}\pmb\theta<\infty\},$
</center>

其中 $\pi(\pmb\theta\mid\pmb x)$ 为 $\theta$ 的后验分布。那么，$\pmb\theta$ 的**Reference先验**定义为在先验分布类 $\mathscr{P}$ 中能够在 $n\rightarrow\infty$ 时最大化期望信息
<center>
$\displaystyle I_{\pi(\pmb\theta)}(\pmb\theta,\pmb x)=\int_{\mathscr{X}}f(\pmb x)\left[\int_{\pmb\Theta}\pi(\pmb\theta\mid\pmb x)\ln\dfrac{\pi(\pmb\theta\mid\pmb x)}{\pi(\pmb\theta)}\right]\text{d}\pmb x$
</center>

的那个先验分布 $\pi^\star(\pmb\theta)=\underset{\pi(\pmb\theta),n\rightarrow\infty}{\arg\max}~I_{\pi(\pmb\theta)}(\pmb\theta,\pmb x)$。

---

当存在多余参数时，假设 $\theta$ 为感兴趣参数，$\lambda$ 为一个多余参数，则Reference先验可以通过如下两步得到：

- 固定 $\theta$，利用前述定义计算Reference先验 $\pi(\lambda\mid\theta)$；
- 如果 $\pi(\lambda\mid\theta)$ 是正常分布，则对 $\lambda$ 积分得到

    <center>
    $\displaystyle f(\pmb x\mid\theta)=\int_{\Lambda}f(\pmb x\mid\theta,\lambda)\pi(\lambda\mid\theta)\text{d}\lambda.$
    </center>
    
    基于 $f(\pmb x\mid\theta)$，利用前述方法得到 $\theta$ 的Reference先验。如果不是正常分布，需要构造一个正常先验的序列，从极限角度进行此过程。
    
对多于两个参数的情形，将感兴趣参数按降序排列，重复利用上述方法。

## 多层先验（分阶段先验）

当所给先验分布中的超参数难以确定时，可以对超参数再给出一个先验，第二个先验称为超先验；若超先验中的超参数还是难以确定时，还可以再给出第三个先验……但大多数情形考虑两层先验就足够了。由先验和超先验决定的一个新先验就称为**分层先验**，也叫做**多阶段先验**。

以两层先验为例，分层先验模型为：
<center>
$\begin{array}{rl}
X\mid\theta \sim &\!\!\!\! f(x\mid\theta),\quad x\in\mathscr{X},\\
\theta\mid\lambda \sim &\!\!\!\! \pi_1(\theta\mid\lambda),\quad\theta\in\Theta,\\
\lambda \sim &\!\!\!\! \pi_2(\lambda),\quad\lambda\in\Lambda.
\end{array}$
</center>

其中 $\mathscr{X}$ 为样本空间，$\Theta$ 为参数空间，$\Lambda$ 为超参数空间。$\pi_2(\lambda)$ 常常取无信息先验。

分层先验模型具有如下特点：

1. 分层先验模型允许在建模时将相对复杂的情形转化为一系列简单的情形；
2. 分层先验模型便于计算。

![](/img/in_post/Bayes/BayesLec5_1_11.png)

## 后记

本文内容参考自中国科学技术大学《贝叶斯分析》（2022Fall）课件。

如对本文内容有任何疑问，请联系 <watthu@mail.ustc.edu.cn>。