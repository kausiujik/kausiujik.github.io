---
layout: post
title: "Bayes-Lec3.2 贝叶斯统计推断（假设检验与预测推断）"
subtitle: "Hypothesis Testing and Prediction"
author: "Watthu"
header-style: text
mathjax: true
tags:
  - 贝叶斯分析
---

## 假设检验

设假设检验问题的一般形式为
<center>
$H:\theta\in\Theta_H\longleftrightarrow K:\theta\in\Theta_K,$
</center>

其中 $\Theta_H\cup\Theta_K=\Theta$，其中 $\Theta$ 是参数空间，$\Theta_H$ 是 $\Theta$ 的非空子集。

在贝叶斯统计推断中，处理假设检验问题是直截了当的，在获得后验分布 $\pi(\theta\mid\pmb x)$ 后，计算两个假设 $H$ 和 $K$ 的后验概率
<center>
$p_H(\pmb x)=\mathbb P(\theta\in\Theta_H\mid\pmb x),\quad p_K(\pmb x)=\mathbb P(\theta\in\Theta_K\mid\pmb x).$
</center>

$p_H(\pmb x)$ 和 $p_K(\pmb x)$ 是综合抽样信息和先验信息得出的两个假设实际发生的后验概率。在做决定时，是通过比较二者的大小进行的：$p_H(\pmb x)<p_K(\pmb x)$ 时就拒绝 $H$，否则接受 $H$。

与经典统计方法相比，贝叶斯统计方法无需选择检验统计量、确定抽样分布；也无需给出检验水平，确定拒绝域；而且容易推广到多重假设检验情形，当有3个或3个以上假设时，应接受具有最大后验概率的假设。

![](/img/in_post/Bayes/BayesLec3_2_1.png)

### 贝叶斯因子

---
**定义(贝叶斯因子)** 设两个假设 $\Theta_0$ 和 $\Theta_1$ 的先验概率分别为 $\pi_0$ 和 $\pi_1$，后验概率分别为 $\alpha_0(\pmb x)$ 和 $\alpha_1(\pmb x)$，比例 $\alpha_0(\pmb x)/\alpha_1(\pmb x)$ 称为 $H_0$ 对 $H_1$ 的**后验优势比**，$\pi_0/\pi_1$ 称为**先验优势比**，则支持 $H_0$ 的**贝叶斯因子**定义为
<center>
$\text{BF}_{01}(\pmb x)=\dfrac{\alpha_0(\pmb x)/\alpha_1(\pmb x)}{\pi_0/\pi_1}.$
</center>

若 $\text{BF}_{01}(\pmb x)$ 取值越大，则对 $H_0$ 的支持程度越高。

---

下面讨论几种不同假设检验情形下的贝叶斯因子。

#### 简单假设对简单假设情形

首先看 $H_0:\theta\in\Theta_0$ 和 $H_1:\theta\in\Theta_1$ 皆为简单假设的情形，即
<center>
$H_0:\theta\in\Theta_0=\{\theta_0\}\longleftrightarrow H_1:\theta\in\Theta_1=\{\theta_1\}.$
</center>

此时，由贝叶斯公式可知
<center>
$\alpha_0(\pmb x)=\mathbb P(\theta\in\Theta_0\mid\pmb x)=\dfrac{f(\pmb x\mid\theta_0)\pi_0}{f(\pmb x\mid\theta_0)\pi_0+f(\pmb x\mid\theta_1)\pi_1},$

$\alpha_1(\pmb x)=\mathbb P(\theta\in\Theta_1\mid\pmb x)=\dfrac{f(\pmb x\mid\theta_1)\pi_1}{f(\pmb x\mid\theta_0)\pi_0+f(\pmb x\mid\theta_1)\pi_1},$
</center>

其中 $f(\pmb x\mid\theta)$ 为样本的分布。从而有贝叶斯因子
<center>
$\text{BF}_{01}(\pmb x)=\dfrac{\alpha_0(\pmb x)/\alpha_1(\pmb x)}{\pi_0/\pi_1}=\dfrac{f(\pmb x\mid\theta_0)}{f(\pmb x\mid\theta_1)}.$
</center>

如果要拒绝假设 $H_0:\theta\in\pmb\Theta_0$，则要求 $\alpha_0(\pmb x)/\alpha_1(\pmb x)<1$, 即要求
<center>
$\dfrac{f(\pmb x\mid\theta_0)}{f(\pmb x\mid\theta_1)}<\dfrac{\pi_1}{\pi_0},$
</center>

这与著名的Neyman-Pearson引理的基本结果类似，从贝叶斯观点看，这个临界值就是两个先验概率比。

因此可见， $\text{BF}_{01}(\pmb x)$ 正是 $H_0\leftrightarrow H_1$ 的似然比，它通常被认为是由数据给出的 $H_0\leftrightarrow H_1$ 的优势比。由于这种情形的贝叶斯因子不依赖于先验分布，仅依赖于样本的似然比，故贝叶斯因子 $\text{BF}_{01}(\pmb x)$ 可视为是数据 $\pmb x$ 支持 $H_0$ 的程度。

![](/img/in_post/Bayes/BayesLec3_2_2.png)

#### 复杂假设对复杂假设情形

考虑下列假设检验问题
<center>
$H_0:\theta\in\Theta_0\longleftrightarrow H_1:\theta\in\Theta_1,$
</center>

其中 $\Theta_0,\Theta_1$ 是参数空间 $\Theta$ 的非空复合集（多元素子集），并且 $\Theta=\Theta_0\cup\Theta_1$。

此时 $\theta$ 的先验分布为
<center>
$\pi(\theta)=\left\{\begin{array}{ll}
	\pi_0g_0(\theta), & \text{当}~\theta\in\Theta_0,\\
	\pi_1g_1(\theta), & \text{当}~\theta\in\Theta_1,
	\end{array}\right.$
</center>

其中 $\pi_0$ 和 $\pi_1$ 分别为 $\Theta_0$ 和 $\Theta_1$ 上的先验概率，$g_0(\theta)$ 和 $g_1(\theta)$ 分别是 $\Theta_0$ 和 $\Theta_1$ 上的概率密度函数。不难验证 $\pi(\theta)$ 是先验密度。事实上，
<center>
$\displaystyle\int_{\Theta}\pi(\theta)\text{d}\theta=\int_{\Theta_0}\pi_0g_0(\theta){\rm d}\theta+\int_{\Theta_1}\pi_1g_1(\theta){\rm d}\theta=\pi_0+\pi_1=1.$
</center>

在上述记号下，后验优势比为
<center>
$\dfrac{\alpha_0(\pmb x)}{\alpha_1(\pmb x)}=\dfrac{\int_{\Theta_0}f(\pmb x\mid\theta)\pi_0g_0(\theta){\rm d}\theta}{\int_{\Theta_1}f(\pmb x\mid\theta)\pi_1g_1(\theta){\rm d}\theta},$
</center>

故贝叶斯因子可表示为
<center>
$\text{BF}_{01}(\pmb x)=\dfrac{\alpha_0(\pmb x)/\alpha_1(\pmb x)}{\pi_0/\pi_1}=\dfrac{\int_{\pmb\Theta_0}f(\pmb x\mid\theta)g_0(\theta){\rm d}\theta}{\int_{\pmb\Theta_1}f(\pmb x\mid\theta)g_1(\theta){\rm d}\theta}=\dfrac{m_0(\pmb x)}{m_1(\pmb x)}.$
</center>

此时不能认为对两个假设支持的度量完全由数据 $\pmb x$ 决定，它还与先验密度 $g_0$ 和 $g_1$ 有关，只能说它部分地消除了先验分布的影响。

![](/img/in_post/Bayes/BayesLec3_2_3.png)

#### 简单假设对复杂假设情形

考虑下列假设检验问题
<center>
$H_0:\theta=\theta_0\longleftrightarrow H_1:\theta\neq\theta_0.$
</center>

这是经典统计中常见的一类检验问题。不过，当参数 $\theta$ 是连续变量时，假设 $H_0$ 是不合理的，一个合理的假设是改上述检验为
<center>
$H_0:\theta\in[\theta_0-\varepsilon,\theta_0+\varepsilon]\longleftrightarrow H_1:\theta\notin[\theta_0-\varepsilon,\theta_0+\varepsilon].$
</center>

其中 $\varepsilon$ 是较小的正数，可选其为误差范围内一个较小的数。

对两个假设的先验给定的一个有效办法是，给 $\theta_0$ 一个正概率 $\pi_0$，而对 $\theta\neq\theta_0$，给一个加权密度 $\pi_1g_1(\theta)$，即 $\theta$ 的先验密度为
<center>
$\pi(\theta)=\left\{\begin{array}{ll}
	\pi_0\delta(\theta-\theta_0), & \text{当}~\theta=\theta_0,\\
	\pi_1g_1(\theta), & \text{当}~\theta\neq\theta_0,
	\end{array}\right.$
</center>

其中 $\pi_0+\pi_1=1$，$\delta(\theta-\theta_0)$ 为脉冲函数，满足
<center>
$\delta(\theta-\theta_0)=\left\{\begin{array}{ll}
	+\infty, & \text{当}~\theta=\theta_0,\\
	0, & \text{当}~\theta\neq\theta_0,
	\end{array}\right.$ 和 $\displaystyle\int_{-\infty}^{+\infty}\delta(\theta-\theta_0){\rm d}\theta=1.$
</center>

在物理学中，脉冲函数 $\delta(\theta-\theta_0)$ 设想为在很小区间 $[\theta_0-\varepsilon,\theta_0+\varepsilon]$ 上具有密度 $1/(2\varepsilon)$，其中 $\varepsilon$ 为很小的正数。

设样本分布为 $f(\pmb x\mid\theta)$，则易求边缘分布为
<center>
$\begin{array}{rl}
\displaystyle m(\pmb x)=&\!\!\!\!\displaystyle\int_{\Theta}f(\pmb x\mid\theta)\pi(\theta){\rm d}\theta\\
=&\!\!\!\!\displaystyle\pi_0f(\pmb x\mid\theta_0)+\int_{\Theta_1}f(\pmb x\mid\theta)\pi_1g_1(\theta){\rm d}\theta\\
=&\!\!\!\!\pi_0f(\pmb x\mid\theta_0)+\pi_1m_1(\pmb x).
\end{array}$
</center>

故 $\{\theta=\theta_0\}$ 和 $\{\theta\neq\theta_1\}$ 的后验优势比为
<center>
$\dfrac{\alpha_0(\pmb x)}{\alpha_1(\pmb x)}=\dfrac{\pi_0f(\pmb x\mid\theta_0)}{\pi_1m_1(\pmb x)},$
</center>

所以贝叶斯因子可表示为
<center>
$\text{BF}_{01}(\pmb x)=\dfrac{\alpha_0(\pmb x)/\alpha_1(\pmb x)}{\pi_0/\pi_1}=\dfrac{f(\pmb x\mid\theta_0)}{m_1(\pmb x)}.$
</center>

实际中我们常常先计算出 $\text{BF}_{01}(\pmb x)$，后计算 $\alpha_0(\pmb x)$ 和 $\alpha_1(\pmb x)$。因为由贝叶斯因子定义和 $\alpha_0(\pmb x)+\alpha_1(\pmb x)=1$ 可推出
<center>
$\alpha_0(\pmb x)=\mathbb P(\theta\in\Theta_0\mid\pmb x)=\left[1+\dfrac{1-\pi_0}{\pi_0}\cdot\dfrac{1}{\text{BF}_{01}(\pmb x)}\right]^{-1},$

$\alpha_1(\pmb x)=\mathbb P(\theta\in\Theta_1\mid\pmb x)=\left[1+\dfrac{\pi_0}{1-\pi_0}\cdot\text{BF}_{01}(\pmb x)\right]^{-1}.$
</center>

![](/img/in_post/Bayes/BayesLec3_2_4.png)

#### 多重假设检验

按贝叶斯分析的观点，多重假设检验并不比两个假设检验更困难，即直接计算每一个假设的后验概率，并比较其大小。

设有如下的**多重假设检验**问题：
<center>
$H_i:\theta\in\Theta_i,\quad i=1,2,\dots,k,$
</center>

其中 $\Theta_1,\Theta_2,\dots,\Theta_k$ 均是参数空间 $\Theta$ 的真子集，且 $\Theta_1\cup\Theta_2\cup\cdots\cup\Theta_k=\Theta$。

通过计算 $\theta$ 落入诸 $\Theta_i$ 的后验概率
<center>
$\displaystyle\alpha_i(\pmb x)=\mathbb P(\theta\in\Theta_i\mid\pmb x)=\int_{\Theta_i}\pi(\theta\mid\pmb x){\rm d}\theta,\quad i=1,2,\dots,k,$
</center>

令 $i_0(\pmb x)=\underset{i}{\arg\max}~\alpha_i(\pmb x)$，接受假设 $H_{i_0(\pmb x)}$。

![](/img/in_post/Bayes/BayesLec3_2_5.png)

## 预测推断

对随机变量的未来预测值作出统计推断称为预测。设 $X\sim f(x\mid\theta)$，令 $\pmb X=(X_1,X_2,\dots,X_n)$ 为从总体 $X$ 中获得的历史数据，大致有以下两种情形：

1. 要对随机变量 $X$ 的未来观测值 $X_0$ 做出推断.
2. 要对具有密度函数为 $g(z\mid\theta)$ 的随机变量 $Z$ 的未来观测值 $Z_0$ 做出推断。这里两个密度函数 $f$ 和 $g$ 具有相同的未知参数 $\theta$，通常假定 $Z$ 和 $X$ 不相关。

贝叶斯预测的想法是：设 $\pi(\theta\mid\pmb x)$ 为 $\theta$ 的后验分布，于是 $g(z\mid\theta)\pi(\theta\mid\pmb x)$ 为给定 $\pmb X=\pmb x$ 条件下 $(Z,\theta)$ 的联合分布，把它对 $\theta$ 积分即可得到给定 $\pmb X=\pmb x$ 条件下 $Z$ 的条件边缘分布，或称为后验预测密度。

---
**定义(后验预测密度)** 设 $X\sim f(x\mid\theta)$，$\pmb X=(X_1,X_2,\dots,X_n)$ 为从总体 $X$ 中获得的历史数据。令 $\pi(\theta)$ 为 $\theta$ 的先验分布，记 $\pi(\theta\mid\pmb x)$ 为 $\theta$ 的后验分布。设随机变量 $Z\sim g(z\mid\theta)$，则给定 $\pmb X=\pmb x$ 后，$Z$ 的未来观测值 $Z_0$ 的**后验预测密度**定义为
<center>
$\displaystyle p(z_0\mid\pmb x)=\int_{\Theta}g(z_0\mid\theta)\pi(\theta\mid\pmb x){\rm d}\theta.$
</center>

特别地，当 $Z$ 和 $X$ 都是同一总体时（此时 $g\equiv f$），则 $X$ 的未来观测值 $X_0$ 的后验预测密度为
<center>
$\displaystyle p(x_0\mid\pmb x)=\int_{\Theta}f(x_0\mid\theta)\pi(\theta\mid\pmb x){\rm d}\theta.$
</center>

---

根据后验预测密度公式可以求出随机变量 $Z$ 的未来观测值 $Z_0$ 的预测值（期望、中位数或众数估计），也可以求出对应可信水平为 $1-\alpha$ 的预测区间 $[a,b]$，满足
<center>
$\displaystyle\mathbb P(a\leq Z_0\leq b\mid\pmb x)=\int_a^bp(z_0\mid\pmb x){\rm d}z_0=1-\alpha.$
</center>

![](/img/in_post/Bayes/BayesLec3_2_6.png)

![](/img/in_post/Bayes/BayesLec3_2_7.png)

## 后记

本文内容参考自中国科学技术大学《贝叶斯分析》（2022Fall）课件。

如对本文内容有任何疑问，请联系 <watthu@mail.ustc.edu.cn>。