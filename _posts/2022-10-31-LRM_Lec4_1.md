---
layout: post
title: "LRM-Lec4_1 参数估计（最小二乘估计与极大似然估计）"
subtitle: "Parameter Estimation (LSE & MLE)"
author: "Watthu"
header-style: text
mathjax: true
tags:
  - 线性统计模型
---

在线性模型参数估计理论与方法中，最小二乘法占有中心的基础地位，本节将重点介绍有关最小二乘估计和极大似然估计的相关理论。

## 最小二乘估计（LSE）

### 最小二乘估计

回顾Lec1中介绍的，我们首先讨论最基本的线性回归模型：
<center>
$\pmb y=\pmb X\pmb\beta+\pmb e,\quad\textsf{E}(\pmb e)=\pmb 0,\quad\textsf{Cov}(\pmb e)=\sigma^2\pmb I_n,$
</center>

其中 $\pmb y$ 为 $n\times 1$ 观测向量，$\pmb X$ 为 $n\times(p+1)$ 已知设计矩阵，$\pmb\beta$ 为 $(p+1)\times 1$ 未知参数向量，$\pmb e$ 是 $n\times1$ 随机误差向量，$\sigma^2>0$ 为误差方差。

我们用**最小二乘法**寻找 $\pmb\beta$ 的估计，其基本思想是使得误差向量 $\pmb e=\pmb y-\pmb X\pmb\beta$ 的长度的平方
<center>
$Q(\pmb\beta):=\|\pmb y-\pmb X\pmb\beta\|^2=(\pmb y-\pmb X\pmb\beta)'(\pmb y-\pmb X\pmb\beta)$
</center>

达到最小。对 $\pmb\beta$ 求导，令其等于零，可得方程组
<center>
$\pmb X'\pmb X\pmb\beta=\pmb X'\pmb y.$
</center>

称之为**正规方程组**（或**正则方程组**）。该方程组有唯一解的充分必要条件是 $\pmb X'\pmb X$ 的秩为 $p+1$，即 $\pmb X$ 是列满秩的。由于 $\pmb X$ 是设计矩阵，可以人为控制，所以我们总假定 $\pmb X$ 是列满秩的，于是可得正规方程组的唯一解
<center>
$\hat{\pmb\beta}=(\pmb X'\pmb X)^{-1}\pmb X'\pmb y.$
</center>

不过，以上讨论只能说明 $\hat{\pmb\beta}$ 是 $Q(\pmb\beta)$ 的一个驻点，进一步，考虑对任意的 $\pmb\beta\in\mathbb R^{p+1}$，有
<center>
$\begin{array}{rl}
\|\pmb y-\pmb X\pmb\beta\|^2=&\!\!\!\!\|\pmb y-\pmb X\hat{\pmb\beta}+\pmb X(\hat{\pmb\beta}-\pmb\beta)\|^2\\
=&\!\!\!\!\|\pmb y-\pmb X\hat{\pmb\beta}\|^2+(\hat{\pmb\beta}-\pmb\beta)'\pmb X'\pmb X(\hat{\pmb\beta}-\pmb\beta)+2(\hat{\pmb\beta}-\pmb\beta)'\pmb X'(\pmb y-\pmb X\hat{\pmb\beta})
\end{array}$
</center>

因为 $\hat{\pmb\beta}$ 满足正规方程组，因此最后一项为零，而第二项由于 $\pmb X'\pmb X$ 是非负定矩阵，所以
<center>
$\|\pmb y-\pmb X\pmb\beta\|^2=\|\pmb y-\pmb X\hat{\pmb\beta}\|^2+(\hat{\pmb\beta}-\pmb\beta)'\pmb X'\pmb X(\hat{\pmb\beta}-\pmb\beta)\geq\|\pmb y-\pmb X\hat{\pmb\beta}\|^2.$
</center>

这就说明了 $\hat{\pmb\beta}$ 确实是 $Q(\pmb\beta)$ 的最小值点。

记 $\hat{\pmb\beta}=(\hat{\beta}_0,\hat{\beta}_1,\dots,\hat{\beta}_p)'$，可以得到经验回归方程
<center>
$\hat{\pmb y}=\pmb X\hat{\pmb\beta}$ 或 $\hat{y}=\hat{\beta}_0+\hat{\beta}_1x_1+\cdots+\hat{\beta}_px_p.$
</center>

回归方程是回归函数的一个估计，但这个方程是不是描述了 $y$ 与 $x_1,\dots,x_p$ 的真实相关关系，还需要作进一步的统计分析。

### 中心化模型与标准化模型

在回归分析中，我们有时把原始数据进行中心化和标准化。令
<center>
$\displaystyle\overline{x}_j=\dfrac{1}{n}\sum_{i=1}^nx_{ij},\quad j=1,\dots,p.$
</center>

将线性回归模型改写为
<center>
$y_i=\alpha+\beta_1(x_{i1}-\overline{x}_1)+\cdots+\beta_p(x_{ip}-\overline{x}_p)+e_i,\quad i=1,\dots,n,$
</center>

这里 $\alpha=\beta_0+\beta_1\overline{x}_1+\cdots+\beta_p\overline{x}_p$，称上式为中心化模型。记
<center>
$\pmb X_c=\left(\begin{array}{cccc}
x_{11}-\overline{x}_1 & x_{12}-\overline{x}_2 & \cdots & x_{1p}-\overline{x}_p\\
x_{21}-\overline{x}_1 & x_{22}-\overline{x}_2 & \cdots & x_{2p}-\overline{x}_p\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1}-\overline{x}_1 & x_{n2}-\overline{x}_2 & \cdots & x_{np}-\overline{x}_p\\
\end{array}\right),$
</center>

则中心化模型可改写为矩阵表示
<center>
$\pmb y=\pmb 1_n\alpha+\pmb X_c\pmb\beta+\pmb e=\left(\pmb 1_n~\pmb X_c\right)\left(\begin{array}{c}
\alpha\\ \pmb\beta
\end{array}\right)+\pmb e,$
</center>

其中 $\pmb\beta=(\beta_1,\dots,\beta_p)'$（注意与之前的不同）。注意到 $\pmb 1_n'\pmb X_c=\pmb 0$，因此正规方程组可写为
<center>
$\left(\begin{array}{cc}
n & \pmb 0\\ \pmb 0 & \pmb X_c'\pmb X_c
\end{array}\right)\left(\begin{array}{c}
\alpha\\ \pmb\beta
\end{array}\right)=\left(\begin{array}{c}
\pmb 1_n'\pmb y\\ \pmb X_c'\pmb y
\end{array}\right).$
</center>

由此得到回归参数的最小二乘估计为
<center>
$\hat{\alpha}=\overline{y},\quad\hat{\pmb\beta}=(\pmb X_c'\pmb X_c)^{-1}\pmb X_c'\pmb y.$
</center>

对于中心化模型，回归常数的LSE总是响应变量的样本均值，而回归系数 $\pmb\beta$ 的LSE等价于从线性回归模型 $\pmb y=\pmb X_c'\pmb\beta+\pmb e$ 中计算得到的LSE。而在实际问题中，计算 $(\pmb X_c'\pmb X_c)^{-1}$ 比计算 $(\pmb X'\pmb X)^{-1}$ 要方便些，且我们大多只关心回归系数，因此中心化模型是实用的。

下面来看标准化。记 $\displaystyle s_j^2=\sum_{i=1}^n(x_{ij}-\overline{x}_j)^2,~j=1,\dots,p$，而作标准化变换
<center>
$z_{ij}=\dfrac{x_{ij}-\overline{x}_j}{s_j},\quad i=1,\dots,n,j=1,\dots,p.$
</center>

令 $\pmb Z=(z_{ij})_{n\times p}$，它具有性质：

- $\pmb 1_n'\pmb Z=\pmb 0$；
- $\pmb R=\pmb Z'\pmb Z=(r_{ij})_{p\times p}$，

其中
<center>
$r_{ij}=\dfrac{\sum\limits_{k=1}^n(x_{ki}-\overline{x}_i)(x_{kj}-\overline{x}_j)}{s_is_j},\quad i,j=1,\dots,p$
</center>

为自变量 $x_i$ 与 $x_j$ 之间的样本相关系数。所以 $\pmb R$ 是自变量的样本相关系数矩阵。经过标准化后的线性回归模型为
<center>
$y_i=\gamma+\dfrac{x_{i1}-\overline{x}_1}{s_1}\beta_1+\cdots+\dfrac{x_{ip}-\overline{x}_p}{s_p}\beta_p+e_i,$
</center>

或其矩阵形式为
<center>
$\pmb y=\pmb 1_n\gamma+\pmb Z\pmb\beta+\pmb e=\left(\pmb 1_n~\pmb Z\right)\left(\begin{array}{c}
\gamma\\ \pmb\beta
\end{array}\right)+\pmb e.$
</center>

进一步可得到回归参数的最小二乘估计为
<center>
$\hat{\gamma}=\overline{y},\quad\hat{\pmb\beta}=(\pmb Z'\pmb Z)^{-1}\pmb Z'\pmb y.$
</center>

对协变量进行标准化的好处在于：

- 可以用来分析回归协变量之间的相关关系；
- 消除了量纲的影响，便于对回归系数估计值的统计分析。

### 最小二乘估计的性质

下面介绍最小二乘估计具有的良好性质。

---
**定理1.** 对于线性回归模型
<center>
$\pmb y=\pmb X\pmb\beta+\pmb e,\quad\textsf{E}(\pmb e)=\pmb 0,\quad\textsf{Cov}(\pmb e)=\sigma^2\pmb I_n,$
</center>

最小二乘估计 $\hat{\pmb\beta}=(\pmb X'\pmb X)^{-1}\pmb X'\pmb y$ 具有下列性质：

- $\textsf{E}(\hat{\pmb\beta})=\pmb\beta$；
- $\textsf{Cov}(\hat{\pmb\beta})=\sigma^2(\pmb X'\pmb X)^{-1}$。

**证明:** 首先 $\textsf{E}(\pmb y)=\pmb X\pmb\beta+\textsf{E}(\pmb e)=\pmb X\pmb\beta$，所以
<center>
$\textsf{E}(\hat{\pmb\beta})=(\pmb X'\pmb X)^{-1}\pmb X'\cdot\textsf{E}(\pmb y)=(\pmb X'\pmb X)^{-1}\pmb X'\pmb X\pmb\beta=\pmb\beta.$
</center>

其次 $\textsf{Cov}(\pmb y)=\textsf{Cov}(\pmb e)=\sigma^2\pmb I_n$，所以
<center>
$\begin{array}{rl}
\textsf{Cov}(\hat{\pmb\beta})=&\!\!\!\!\textsf{Cov}((\pmb X'\pmb X)^{-1}\pmb X'\pmb y)\\
=&\!\!\!\!(\pmb X'\pmb X)^{-1}\pmb X'\textsf{Cov}(\pmb y)\pmb X(\pmb X'\pmb X)^{-1}\\
=&\!\!\!\!(\pmb X'\pmb X)^{-1}\pmb X'\sigma^2\pmb I_n\pmb X(\pmb X'\pmb X)^{-1}=\sigma^2(\pmb X'\pmb X)^{-1}.
\end{array}$
</center>

因此定理得证。

**推论.** 设 $\pmb c$ 是 $(p+1)\times1$ 常数向量，对于线性函数 $\pmb c'\pmb\beta$，
<center>
$\textsf{E}(\pmb c'\hat{\pmb\beta})=\pmb c'\pmb\beta,\quad\textsf{Cov}(\pmb c'\hat{\pmb\beta})=\sigma^2\pmb c'(\pmb X'\pmb X)^{-1}\pmb c.$
</center>

特别地，取 $\pmb c=(0,\dots,0,1,0,\dots,0)'$，其中 $1$ 在 $\pmb c$ 的第 $i+1$ 个位置，则 $\pmb c'\pmb\beta=\beta_i$，$\pmb c'\hat{\pmb\beta}=\hat{\beta_i}$。记 $(\pmb A)_{ii}$ 表示矩阵 $\pmb A$ 的第 $(i,i)$ 元素，则
<center>
$\textsf{E}(\hat{\beta}_i)=\beta_i,\quad\textsf{Var}(\hat{\beta}_i)=\sigma^2((\pmb X'\pmb X)^{-1})_{i+1,i+1}.$
</center>

---

从推论中可以看到 $\pmb c'\hat{\pmb\beta}$ 是 $\pmb c'\pmb\beta$ 的线性无偏估计。当然，利用其它估计方法（如极大似然估计（但需假设随机误差向量的分布）等）也可以得到 $\pmb c'\pmb\beta$ 的无偏估计，这就构成了 $\pmb c'\pmb\beta$ 的无偏估计类。

---
**定理2.(Gauss-Markov)** 对于线性回归模型
<center>
$\pmb y=\pmb X\pmb\beta+\pmb e,\quad\textsf{E}(\pmb e)=\pmb 0,\quad\textsf{Cov}(\pmb e)=\sigma^2\pmb I_n,$
</center>

在 $\pmb c'\pmb\beta$ 的所有线性无偏估计中，最小二乘估计 $\pmb c'\hat{\pmb\beta}$ 是唯一的**最小方差线性无偏估计（Best Linear Unbiased Estimator, BLUE）**。

**证明:** 设 $\pmb a'\pmb y$ 为 $\pmb c'\pmb\beta$ 的一个线性无偏估计。于是对任意 $p+1$ 维列向量 $\pmb\beta$，有
<center>
$\pmb c'\pmb\beta=\textsf{E}(\pmb a'\pmb y)=\pmb a'\pmb X\pmb\beta,$
</center>

因此 $\pmb a'\pmb X=\pmb c'$。考虑到 $\textsf{Var}(\pmb a'\pmb y)=\sigma^2\pmb a'\pmb a$，我们对 $\pmb a'\pmb a$ 作分解：
<center>
$\begin{array}{rl}
\pmb a'\pmb a=&\!\!\!\!\|\pmb a-\pmb X(\pmb X'\pmb X)^{-1}\pmb c+\pmb X(\pmb X'\pmb X)^{-1}\pmb c\|^2\\
=&\!\!\!\!\|\pmb a-\pmb X(\pmb X'\pmb X)^{-1}\pmb c\|^2+\|\pmb X(\pmb X'\pmb X)^{-1}\pmb c\|^2+2\pmb c'(\pmb X'\pmb X)^{-1}\pmb X'(\pmb a-\pmb X(\pmb X'\pmb X)^{-1}\pmb c)\\
=&\!\!\!\!\|\pmb a-\pmb X(\pmb X'\pmb X)^{-1}\pmb c\|^2+\|\pmb X(\pmb X'\pmb X)^{-1}\pmb c\|^2+2\pmb c'(\pmb X'\pmb X)^{-1}(\pmb X'\pmb a-\pmb c)\\
=&\!\!\!\!\|\pmb a-\pmb X(\pmb X'\pmb X)^{-1}\pmb c\|^2+\|\pmb X(\pmb X'\pmb X)^{-1}\pmb c\|^2\\
=&\!\!\!\!\|\pmb a-\pmb X(\pmb X'\pmb X)^{-1}\pmb c\|^2+\pmb c'(\pmb X'\pmb X)^{-1}\pmb c
\end{array}$
</center>

因此
<center>
$\begin{array}{rl}
\textsf{Var}(\pmb a'\pmb y)=&\!\!\!\!\sigma^2\|\pmb a-\pmb X(\pmb X'\pmb X)^{-1}\pmb c\|^2+\sigma^2\pmb c'(\pmb X'\pmb X)^{-1}\pmb c\\
=&\!\!\!\!\sigma^2\|\pmb a-\pmb X(\pmb X'\pmb X)^{-1}\pmb c\|^2+\textsf{Var}(\pmb c'\hat{\pmb\beta})\geq\textsf{Var}(\pmb c'\hat{\pmb\beta}).
\end{array}$
</center>

等号成立当且仅当 $\pmb a=\pmb X(\pmb X'\pmb X)^{-1}\pmb c$，此时 $\pmb a'\pmb y=\pmb c'\hat{\pmb\beta}$，定理得证。

**推论.** 在 $\beta_i,~i=1,\dots,p$ 的一切线性无偏估计中，$\hat{\beta}_i,~i=1,\dots,p$ 是唯一的方差最小者。

---

将定理1应用到中心化模型，则有如下推论。

---
**推论.** 对于中心化模型
<center>
$\pmb y=\pmb 1_n\alpha+\pmb X_c\pmb\beta+\pmb e=\left(\pmb 1_n~\pmb X_c\right)\left(\begin{array}{c}
\alpha\\ \pmb\beta
\end{array}\right)+\pmb e,\quad\textsf{E}(\pmb e)=\pmb 0,\quad\textsf{Cov}(\pmb e)=\sigma^2\pmb I_n,$
</center>

这里 $\pmb\beta=(\beta_1,\dots,\beta_p)'$，则有
<center>
$\textsf{E}\left(\begin{array}{c}
\hat{\alpha}\\ \hat{\pmb\beta}
\end{array}\right)=\left(\begin{array}{c}
\alpha\\ \pmb\beta
\end{array}\right),\quad\textsf{Cov}\left(\begin{array}{c}
\hat{\alpha}\\ \hat{\pmb\beta}
\end{array}\right)=\sigma^2\left(\begin{array}{cc}
\dfrac{1}{n} & \pmb 0\\ \pmb 0 & (\pmb X_c'\pmb X_c)^{-1}
\end{array}\right),$
</center>

其中 $\hat{\alpha}=\overline{y}$，$\hat{\pmb\beta}=(\pmb X_c'\pmb X_c)^{-1}\pmb X_c'\pmb y$。

---

## 极大似然估计（MLE）

如果在线性回归模型
<center>
$\pmb y=\pmb X\pmb\beta+\pmb e,\quad\textsf{E}(\pmb e)=\pmb 0,\quad\textsf{Cov}(\pmb e)=\sigma^2\pmb I_n$
</center>

中假设随机误差向量服从正态分布，则得到正态线性回归模型
<center>
$\pmb y=\pmb X\pmb\beta+\pmb e,\quad\pmb e\sim N(\pmb 0,\sigma^2\pmb I_n).$
</center>

---
**定理3.** 对于正态线性回归模型
<center>
$\pmb y=\pmb X\pmb\beta+\pmb e,\quad\pmb e\sim N(\pmb 0,\sigma^2\pmb I_n),$
</center>

有

- $\hat{\pmb\beta}=(\pmb X'\pmb X)^{-1}\pmb X'\pmb y$ 是 $\pmb\beta$ 的极大似然估计；
- $\hat{\pmb\beta}\sim N(\pmb\beta,\sigma^2(\pmb X'\pmb X)^{-1})$；

**证明:** 考虑 $\pmb\beta$ 的对数似然函数，注意到 $\pmb y\sim N(\pmb X\pmb\beta,\sigma^2\pmb I_n)$，所以
<center>
$l(\pmb\beta)=-\dfrac{n}{2}\ln(2\pi\sigma^2)-\dfrac{1}{2\sigma^2}\|\pmb y-\pmb X\pmb\beta\|^2,$
</center>

在固定 $\sigma^2$ 的条件下，最大化对数似然等价于最小化 $\|\pmb y-\pmb X\pmb\beta\|^2$，这与最小二乘法的目标函数相同，因此最小二乘估计 $\hat{\pmb\beta}=(\pmb X'\pmb X)^{-1}\pmb X'\pmb y$ 也是 $\pmb\beta$ 的极大似然估计。而 $\pmb y\sim N(\pmb X\pmb\beta,\sigma^2\pmb I_n)$，于是
<center>
$\hat{\pmb\beta}=(\pmb X'\pmb X)^{-1}\pmb X'\pmb y\sim N(\pmb\beta,\sigma^2(\pmb X'\pmb X)^{-1}).$
</center>

这也可以从定理1直接导出，因为多元正态分布完全由其均值向量和协方差阵唯一决定。

**推论.** 设 $\pmb c$ 是 $(p+1)\times1$ 常数向量，对于线性函数 $\pmb c'\pmb\beta$，
<center>
$\pmb c'\hat{\pmb\beta}\sim N(\pmb c'\pmb\beta,\sigma^2\pmb c'(\pmb X'\pmb X)^{-1}\pmb c).$
</center>

特别地，取 $\pmb c=(0,\dots,0,1,0,\dots,0)'$，其中 $1$ 在 $\pmb c$ 的第 $i+1$ 个位置，则 $\pmb c'\pmb\beta=\beta_i$，$\pmb c'\hat{\pmb\beta}=\hat{\beta_i}$。记 $(\pmb A)_{ii}$ 表示矩阵 $\pmb A$ 的第 $(i,i)$ 元素，则
<center>
$\hat{\beta}_i\sim N(\beta_i,\sigma^2((\pmb X'\pmb X)^{-1})_{i+1,i+1}).$
</center>

**推论.** 对于中心化正态模型
<center>
$\pmb y=\pmb 1_n\alpha+\pmb X_c\pmb\beta+\pmb e=\left(\pmb 1_n~\pmb X_c\right)\left(\begin{array}{c}
\alpha\\ \pmb\beta
\end{array}\right)+\pmb e,\quad\pmb e\sim N(\pmb 0,\sigma^2\pmb I_n),$
</center>

这里 $\pmb\beta=(\beta_1,\dots,\beta_p)'$，则有
<center>
$\hat{\alpha}\sim N\left(\alpha,\dfrac{\sigma^2}{n}\right),\quad\hat{\pmb\beta}\sim N(\pmb\beta,\sigma^2(\pmb X_c'\pmb X_c)^{-1}),$
</center>

且 $\hat{\alpha}$ 与 $\pmb\beta$ 相互独立，其中 $\hat{\alpha}=\overline{y}$，$\hat{\pmb\beta}=(\pmb X_c'\pmb X_c)^{-1}\pmb X_c'\pmb y$。

---

**定理4.** 对于正态线性回归模型
<center>
$\pmb y=\pmb X\pmb\beta+\pmb e,\quad\pmb e\sim N(\pmb 0,\sigma^2\pmb I_n),$
</center>

有

- $T_1=\pmb y'\pmb y$ 和 $\pmb T_2=\pmb X'\pmb y$ 为充分完全统计量；
- 对任一可估函数 $\pmb c'\pmb\beta$，$\pmb c'\hat{\pmb\beta}$ 为其唯一的最小方差无偏估计。

**证明:** 注意到 $\pmb y\sim N(\pmb X\pmb\beta,\sigma^2\pmb I_n)$，所以其概率密度函数为
<center>
$\begin{array}{rl}
f(\pmb y)=&\!\!\!\!\dfrac{1}{(2\pi\sigma^2)^{n/2}}\exp\left\{-\dfrac{1}{2\sigma^2}(\pmb y-\pmb X\pmb\beta)'(\pmb y-\pmb X\pmb\beta)\right\}\\
=&\!\!\!\!\dfrac{1}{(2\pi\sigma^2)^{n/2}}\exp\left\{-\dfrac{1}{2\sigma^2}\pmb y'\pmb y+\dfrac{1}{\sigma^2}\pmb y'\pmb X\pmb\beta-\dfrac{1}{2\sigma^2}\pmb\beta'\pmb X'\pmb X\pmb\beta\right\},
\end{array}$
</center>

记 $\theta_1=-\dfrac{1}{2\sigma^2}$，$\pmb\theta_2=\dfrac{\pmb\beta}{\sigma^2}$ 为自然参数，则上式可改写为指数族的自然形式
<center>
$f(\pmb y)=\Big(\dfrac{-\theta_1}{\pi}\Big)^{n/2}\text{e}^{\frac{1}{4\theta_1}\pmb\theta_2'\pmb X'\pmb X\pmb\theta_2}\exp\{\theta_1T_1+\pmb\theta_2'\pmb T_2\}.$
</center>

由于参数空间 $\pmb\Theta=\{(\theta_1,\pmb\theta_2')':\theta_1<0,\pmb\theta_2\in\mathbb R^p\}$ 有内点，因此 $T_1=\pmb y'\pmb y$ 和 $\pmb T_2=\pmb X'\pmb y$ 为充分完全统计量。

由于 $\pmb c'\hat{\pmb\beta}=\pmb c'\pmb X(\pmb X'\pmb X)^{-1}\pmb X'\pmb y=\pmb c'\pmb X(\pmb X'\pmb X)^{-1}\pmb T_2$ 是充分完全统计量的函数，且其为 $\pmb c'\pmb\beta$ 的无偏估计，因此它是最小方差无偏估计。

**注：** 回顾称 $\pmb c'\pmb\beta$ 是可估函数，若存在 $n\times 1$ 向量 $\pmb a$，使得 $\textsf{E}(\pmb a'\pmb y)=\pmb c'\pmb\beta$ 对一切 $\pmb\beta$ 成立。

## 模型的解释能力

现在我们讨论模型中的另一个重要参数：误差方差 $\sigma^2$，它反映了模型误差的大小，在回归分析中起着很重要的作用。

首先讨论它的估计。由于 $\pmb e=\pmb y-\pmb X\pmb\beta$ 是误差向量，不可观测，考虑用 $\hat{\pmb\beta}$ 代替 $\pmb\beta$，称
<center>
$\hat{\pmb e}=\pmb y-\pmb X\hat{\pmb\beta}=\pmb y-\hat{\pmb y}:=(\pmb I_n-\pmb P_{\pmb X})\pmb y$
</center>

为**残差（residual）向量**。自然的，我们可以将 $\hat{\pmb e}$ 看作 $\pmb e$ 的一个估计，用
<center>
$\displaystyle\text{RSS}=\hat{\pmb e}'\hat{\pmb e}=\sum_{i=1}^n\hat{e}_i^2$
</center>

作为 $\sigma^2$ 的估计，这里 $\hat{e}_i$ 是第 $i$ 次观测的误差。RSS（Residual Sum of Squares）表示**残差平方和**，它反映了观测数据与回归直线的偏离程度，其值越小表明数据与模型的吻合度越高。

---
**定理5.** 对于线性回归模型
<center>
$\pmb y=\pmb X\pmb\beta+\pmb e,\quad\textsf{E}(\pmb e)=\pmb 0,\quad\textsf{Cov}(\pmb e)=\sigma^2\pmb I_n,$
</center>

有

- $\text{RSS}=\pmb y'(\pmb I_n-\pmb X(\pmb X'\pmb X)^{-1}\pmb X')\pmb y=:\pmb y'(\pmb I_n-\pmb P_{\pmb X})\pmb y$；
- $\hat{\sigma}^2=\dfrac{\text{RSS}}{n-r}$ 是 $\sigma^2$ 的无偏估计量，其中 $r=\textsf{rk}(\pmb X)=p+1$。

**证明:** 由RSS的定义
<center>
$\begin{array}{rl}
\text{RSS}=&\!\!\!\!\hat{\pmb e}'\hat{\pmb e}=(\pmb y-\pmb X\hat{\pmb\beta})'(\pmb y-\pmb X\hat{\pmb\beta})\\
=&\!\!\!\![(\pmb I_n-\pmb X(\pmb X'\pmb X)^{-1}\pmb X')\pmb y]'[(\pmb I_n-\pmb X(\pmb X'\pmb X)^{-1}\pmb X')\pmb y]\\
=&\!\!\!\!\pmb y'(\pmb I_n-\pmb X(\pmb X'\pmb X)^{-1}\pmb X')\pmb y.
\end{array}$
</center>

而由于 $\textsf{E}(\pmb y)=\pmb X\pmb\beta$，$\textsf{Cov}(\pmb y)=\sigma^2\pmb I_n$，以及Lec3\_1定理5（随机向量的二次型的期望）可知
<center>
$\begin{array}{rl}
\textsf{E}(\text{RSS})=&\!\!\!\!\textsf{E}[\pmb y'(\pmb I_n-\pmb P_{\pmb X})\pmb y]\\
=&\!\!\!\!\pmb\beta'\pmb X'(\pmb I_n-\pmb P_{\pmb X})\pmb X\pmb\beta+\sigma^2\textsf{tr}(\pmb I_n-\pmb P_{\pmb X})\\
=&\!\!\!\!\sigma^2[n-\textsf{tr}((\pmb X'\pmb X)^{-1}\pmb X'\pmb X)]\\
=&\!\!\!\!\sigma^2[n-\textsf{tr}(\pmb I_{p+1})]=\sigma^2[n-(p+1)].
\end{array}$
</center>

由此定理得证。

**注：** $\pmb P_{\pmb X}=\pmb X(\pmb X'\pmb X)^{-1}\pmb X'$ 称为正交投影阵，它是对称幂等矩阵。此外，不难验证
<center>
$\textsf{E}(\hat{\pmb e})=(\pmb I_n-\pmb P_{\pmb X})\textsf{E}(\pmb y)=\pmb X\pmb\beta-\pmb X(\pmb X'\pmb X)^{-1}\pmb X'\pmb X\pmb\beta=\pmb 0,\\
\textsf{Cov}(\hat{\pmb e})=(\pmb I_n-\pmb P_{\pmb X})\textsf{Cov}(\pmb y)(\pmb I_n-\pmb P_{\pmb X})'=\sigma^2(\pmb I_n-\pmb P_{\pmb X}).$
</center>

另外，事实上，$\hat{\sigma}^2$ 是 $\sigma^2$ 的最小方差无偏估计，因为
<center>
$\hat{\sigma}^2=\dfrac{\text{RSS}}{n-r}=\dfrac{1}{n-r}(T_1-\pmb T_2'(\pmb X'\pmb X)^{-1}\pmb T_2)$
</center>

是充分完备统计量的函数。

---

**定理6.** 对于正态线性回归模型
<center>
$\pmb y=\pmb X\pmb\beta+\pmb e,\quad\pmb e\sim N(\pmb 0,\sigma^2\pmb I_n),$
</center>

有

- $\tilde{\sigma}^2=\dfrac{\text{RSS}}{n}$ 是 $\sigma^2$ 的极大似然估计量，它不是 $\sigma^2$ 的无偏估计量；
- $\dfrac{\text{RSS}}{\sigma^2}\sim\chi^2(n-r)$，其中 $r=\textsf{rk}(\pmb X)=p+1$；
- $\hat{\pmb\beta}$ 与 $\text{RSS}$ 相互独立。

**证明:** 由定理3的证明中考虑 $(\pmb\beta,\sigma^2)$ 的联合对数似然函数对参数求偏导并令其为零，则 $\sigma^2$ 的极大似然估计为
<center>
$\dfrac{1}{n}\|\pmb y-\pmb X\hat{\pmb\beta}\|^2=\dfrac{\text{RSS}}{n},$
</center>

它不是 $\sigma^2$ 的无偏估计量。

对于涉及分布与独立性问题，回顾正态随机向量的二次型的相关结论。根据定义
<center>
$\text{RSS}=\pmb y'(\pmb I_n-\pmb P_{\pmb X})\pmb y=(\pmb X\pmb\beta+\pmb e)'(\pmb I_n-\pmb P_{\pmb X})(\pmb X\pmb\beta+\pmb e)=\pmb e'(\pmb I_n-\pmb P_{\pmb X})\pmb e.$
</center>

这里用到了 $(\pmb I_n-\pmb P_{\pmb X})\pmb X=\pmb 0$，而 $\pmb e\sim N(\pmb 0,\sigma^2\pmb I_n)$，由Lec3\_2定理2的推论，只需证明
<center>
$\pmb I_n-\pmb P_{\pmb X}$ 幂等，且 $\textsf{rk}(\pmb I_n-\pmb P_{\pmb X})=n-r.$
</center>

由定理4的证明过程可知，结合 $\textsf{rk}(\pmb I_n-\pmb P_{\pmb X})=\textsf{tr}(\pmb I_n-\pmb P_{\pmb X})$ 可知上述结论成立，因此 $\dfrac{\text{RSS}}{\sigma^2}\sim\chi^2(n-r)$。

而 $\hat{\pmb\beta}=\pmb\beta+(\pmb X'\pmb X)^{-1}\pmb X'\pmb e$，$\text{RSS}=\pmb e'(\pmb I_n-\pmb P_{\pmb X})\pmb e$，于是由
<center>
$(\pmb X'\pmb X)^{-1}\pmb X'\sigma^2\pmb I_n(\pmb I_n-\pmb P_{\pmb X})=\pmb 0,$
</center>

结合Lec3\_2定理4的推论知二次型 $\text{RSS}=\pmb e'(\pmb I_n-\pmb P_{\pmb X})\pmb e$ 与线性型 $(\pmb X'\pmb X)^{-1}\pmb X'\pmb e$ 相互独立，从而 $\hat{\pmb\beta}$ 与 $\text{RSS}$ 相互独立。

**注：** 特别地，进一步也可以得到 $\pmb c'\hat{\pmb\beta}$ 与 $\tilde{\sigma}^2$（或 $\hat{\sigma}^2$）相互独立。


---

前面已经提到，RSS的值表明了数据与模型的吻合程度，下面介绍一个衡量模型解释能力的量——判定系数 $R^2$。

类似于RSS的思想，定义
<center>
$\displaystyle\text{ESS}=\sum_{i=1}^n(\hat{y}_i-\overline{y})^2=(\hat{\pmb y}-\pmb 1_n\overline{y})'(\hat{\pmb y}-\pmb 1_n\overline{y})$
</center>

为**回归平方和**（或**解释平方和**，Explained Sum of Squares），
<center>
$\displaystyle\text{TSS}=\sum_{i=1}^n(y_i-\overline{y})^2=(\pmb y-\pmb 1_n\overline{y})'(\pmb y-\pmb 1_n\overline{y})$
</center>

为**总偏差平方和**（或**总平方和**，Total Sum of Squares）。由正规方程组可以证明
<center>
$\begin{array}{rl}
\displaystyle\text{TSS}=\sum_{i=1}^n(y_i-\overline{y})^2=&\!\!\!\!\displaystyle\sum_{i=1}^n(y_i-\hat{y}_i+\hat{y_i}-\overline{y})^2\\
=&\!\!\!\!\displaystyle\sum_{i=1}^n(y_i-\hat{y}_i)^2+\sum_{i=1}^n(\hat{y}_i-\overline{y})^2+2\sum_{i=1}^n(y_i-\hat{y}_i)(\hat{y}_i-\overline{y})\\
=&\!\!\!\!\displaystyle\sum_{i=1}^n(y_i-\hat{y}_i)^2+\sum_{i=1}^n(\hat{y}_i-\overline{y})^2=\text{RSS}+\text{ESS}.
\end{array}$
</center>

由此定义**判定系数**
<center>
$R^2=\dfrac{\text{ESS}}{\text{TSS}},$
</center>

它度量了回归协变量 $x_1,\dots,x_p$ 对像响应变量 $y$ 的解释能力，$0\leq R^2\leq 1$，其值越接近1，意味着解释能力越强。此外，称 $R=\sqrt{R^2}$ 为**复相关系数**。

对于中心化模型，由于 $\hat{\pmb y}-\pmb 1_n\overline{y}=\hat{\pmb y}-\pmb 1_n\hat{\alpha}=\pmb X_c\hat{\pmb\beta}$，所以
<center>
$\text{ESS}=(\hat{\pmb y}-\pmb 1_n\overline{y})'(\hat{\pmb y}-\pmb 1_n\overline{y})=\hat{\pmb\beta}\pmb X_c'\pmb X_c\hat{\pmb\beta}=\hat{\pmb\beta}\pmb X_c'\pmb y.$
</center>

## 后记

本文内容参考自《线性模型引论》（王松桂等，科学出版社）。

如对本文内容有任何疑问，请联系 <watthu@mail.ustc.edu.cn>。